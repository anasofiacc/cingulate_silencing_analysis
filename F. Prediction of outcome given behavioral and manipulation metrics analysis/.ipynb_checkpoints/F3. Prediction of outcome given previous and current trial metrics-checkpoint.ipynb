{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "entire-moral",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%run \"C:\\Users\\anasofiaccruz\\Desktop\\cingulate_silencing_repo\\data_prep_functions.ipynb\"\n",
    "%run \"C:\\Users\\anasofiaccruz\\Desktop\\cingulate_silencing_repo\\data_analysis_functions.ipynb\"\n",
    "%run \"C:\\Users\\anasofiaccruz\\Desktop\\cingulate_silencing_repo\\Inference_testing_functions.ipynb\"\n",
    "%run Trial_history_calculations_functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-powell",
   "metadata": {},
   "source": [
    "#### <font color='darkorange'> Read the data (test runs below 15 seconds) and remove baseline trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "bulgarian-terrace",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "below15 = read_below15_and_remove_baseline_trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "pressed-colon",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def split_into_train_and_test(df):\n",
    "    \n",
    "    '''\n",
    "    Shuffles df rows and splits into a train (80%) and test set (20%)\n",
    "        Arg1, df, Pandas DataFrame \n",
    "    Returns:\n",
    "        test - df, Pandas DataFrame\n",
    "        train - df, Pandas DataFrame\n",
    "    \n",
    "    '''\n",
    "    df = df.sample(frac=1)\n",
    "    train = df.sample(frac=.8, random_state=1)\n",
    "    test = df.drop(train.index, axis=0)\n",
    "    \n",
    "    return test, train\n",
    "\n",
    "def create_dummies(df,cols):\n",
    "    '''\n",
    "    Create dummies for cols in df\n",
    "        Arg1, df, Pandas DataFrame\n",
    "        Arg2, cols, list - List of features to dummify\n",
    "    Return:\n",
    "        df, Pandas DataFrame\n",
    "    '''\n",
    "    \n",
    "    for col in cols:\n",
    "        dummies = pd.get_dummies(df[col], drop_first=True, prefix=col)\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "    print('bla')\n",
    "    df = df.drop(cols, axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "terminal-transsexual",
   "metadata": {},
   "source": [
    "### Create shifted (previous trial) features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "consolidated-maintenance",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''cols_to_shift = ['stim_condition', 'latency_to_cp_entry', 'time_in_cp', 'outcome']\n",
    "\n",
    "for col in cols_to_shift:\n",
    "    below15 = create_var_shifted_column(below15, col, 1).rename(columns={'shifted1':'prev_'+col})'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "abandoned-indicator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prev_stim_condition         244\n",
       "prev_latency_to_cp_entry    244\n",
       "prev_time_in_cp             244\n",
       "prev_outcome                244\n",
       "dtype: int64"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First trials of each session do not have info regarding the previous trial. These will be removed\n",
    "#below15[['prev_stim_condition','prev_latency_to_cp_entry', 'prev_time_in_cp', 'prev_outcome']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "educated-plymouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "#below15 = below15.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "modified-license",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CTRL'] ['NPHR']\n"
     ]
    }
   ],
   "source": [
    "subset = below15[['group', 'stim_condition', 'latency_to_cp_entry', 'time_in_cp', 'outcome',\n",
    "                 'prev_stim_condition', 'prev_latency_to_cp_entry', \n",
    "                 'prev_time_in_cp', 'prev_outcome']]\n",
    "subset.head()\n",
    "\n",
    "ctrl = subset[subset['group']=='CTRL']\n",
    "nphr = subset[subset['group']=='NPHR']\n",
    "print(ctrl['group'].unique(), nphr['group'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brutal-occurrence",
   "metadata": {},
   "source": [
    "### Split data into a test and training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "several-spiritual",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrl_test, ctrl_train = split_into_train_and_test(ctrl)\n",
    "nphr_test, nphr_train = split_into_train_and_test(nphr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "thrown-yorkshire",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NPHR', 'CTRL'], dtype=object)"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nphr_train['group'].unique()\n",
    "ctrl_train['group'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classical-columbus",
   "metadata": {},
   "source": [
    "#### Categorize latencies and times according to quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "cloudy-warehouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_quantiles(df, cols):\n",
    "    \n",
    "    for col in cols:\n",
    "        df[col+'_quantiles']=pd.qcut(df[col], q=4, labels=[1,2,3,4])\n",
    "        df[col+'_quantiles'] = df[col+'_quantiles'].astype('category')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "overall-magnitude",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bla\n",
      "bla\n"
     ]
    }
   ],
   "source": [
    "cols_to_dummify=['outcome',\n",
    "                 'latency_to_cp_entry_quantiles',\n",
    "                 'time_in_cp_quantiles']\n",
    "\n",
    "for df in [ctrl_train, nphr_train]:\n",
    "    df = categorize_quantiles(df, ['latency_to_cp_entry', 'time_in_cp'])\n",
    "\n",
    "ctrl_train = create_dummies(ctrl_train, cols_to_dummify)\n",
    "nphr_train = create_dummies(nphr_train, cols_to_dummify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "deadly-effort",
   "metadata": {},
   "outputs": [],
   "source": [
    "features=['latency_to_cp_entry_quantiles_2', 'latency_to_cp_entry_quantiles_3',\n",
    "          'latency_to_cp_entry_quantiles_4', 'time_in_cp_quantiles_2',\n",
    "          'time_in_cp_quantiles_3', 'time_in_cp_quantiles_4', 'outcome_1.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "bored-edward",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from sklearn.linear_model import LogisticRegression\\ngrid={'penalty':['l1', 'l2'],\\n      'C':[1,3,5,8,10]}\\nlr = LogisticRegression(random_state=1, class_weight='balanced')\\nbest = perform_a_grid_search(lr, grid, 5, train, features, 'stim_condition')\\nbest\""
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from sklearn.linear_model import LogisticRegression\n",
    "grid={'penalty':['l1', 'l2'],\n",
    "      'C':[1,3,5,8,10]}\n",
    "lr = LogisticRegression(random_state=1, class_weight='balanced')\n",
    "best = perform_a_grid_search(lr, grid, 5, train, features, 'stim_condition')\n",
    "best'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "coated-concord",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6277    NPHR\n",
       "4224    CTRL\n",
       "2840    NPHR\n",
       "1780    NPHR\n",
       "6865    NPHR\n",
       "        ... \n",
       "7237    NPHR\n",
       "3710    NPHR\n",
       "1928    NPHR\n",
       "5474    CTRL\n",
       "3786    CTRL\n",
       "Name: group, Length: 4806, dtype: object"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nphr_train['group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "driven-publisher",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34186070686070685\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=1)\n",
    "scores=cross_val_score(lr, ctrl_train[features], ctrl_train['stim_condition'], cv=10)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "impaired-kelly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34186070686070685\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=1)\n",
    "scores=cross_val_score(lr, nphr_train[features], nphr_train['stim_condition'], cv=10)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opening-warehouse",
   "metadata": {},
   "source": [
    "### Perform a grid search of hyperparameters for random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-tragedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def perform_a_grid_search(estimator, parameters_dict, cv, df, features, target):\n",
    "    '''\n",
    "    Grid Search best parameters for the estimator.\n",
    "    Arg1, estimator - model instance\n",
    "    Arg2, parameters_dict, dict \n",
    "    Arg3, cv, int - number of folds\n",
    "    Arg4, df, Pandas DataFrame\n",
    "    Arg5, features, list\n",
    "    Arg6, target, str\n",
    "    Return:\n",
    "        estimator with best parameters\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    grid=GridSearchCV(estimator, parameters_dict, cv=cv)\n",
    "    grid.fit(df[features], df[target])\n",
    "    \n",
    "    return grid.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affecting-uzbekistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "grid={'criterion':['gini','entropy'],\n",
    "      'n_estimators':[3,5,10],\n",
    "      'max_depth': [5,10,15,20],\n",
    "      'min_samples_leaf': [5,10,15,20,25]}\n",
    "\n",
    "best = perform_a_grid_search(rf, grid, 5, train, features, 'stim_condition')\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-closure",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "rfc = RandomForestClassifier(\n",
    "            criterion='gini',\n",
    "            max_depth= 10,\n",
    "            min_samples_leaf=5,\n",
    "            n_estimators=10\n",
    ")\n",
    "\n",
    "scores=cross_val_score(rfc, train[features], train['stim_condition'], cv=10)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "falling-luxury",
   "metadata": {},
   "source": [
    "### Perform a grid search of hyperparameters for KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-familiar",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "grid={'n_neighbors':[5,10,20,30,50, 100,150]}\n",
    "knn = KNeighborsClassifier()\n",
    "best = perform_a_grid_search(knn, grid, 5, train, features, 'stim_condition')\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-challenge",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=100)\n",
    "scores=cross_val_score(knn, train[features], train['stim_condition'], cv=10)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-retreat",
   "metadata": {},
   "source": [
    "### Perform a grid search of hyperparameters for LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-edward",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "grid={'penalty':['l1', 'l2'],\n",
    "      'C':[1,3,5,8,10]}\n",
    "lr = LogisticRegression(random_state=1, class_weight='balanced')\n",
    "best = perform_a_grid_search(lr, grid, 5, train, features, 'stim_condition')\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-chemical",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=1, C=3, penalty='l2')\n",
    "scores=cross_val_score(lr, train[features], train['outcome'], cv=10)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "natural-sphere",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
